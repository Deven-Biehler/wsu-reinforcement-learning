{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "problem_description",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Ocfmg_VCJ5fS"
      },
      "source": [
        "# Homework 4 (coding part)\n",
        "\n",
        "## Bipedal Walker Problem - Twin Delayed DDPG (TD3)\n",
        "\n",
        "In this assignment, we will solve the \"BipedalWalker-v3\" task from the OpenAI Gym using Twin Delayed DDPG (TD3) algorithm.\n",
        "\n",
        "### Problem Desciption\n",
        "\n",
        "In this environment, a 2D bipedal walker has to learn a policy to walk without falling over. The total reward calculation is based on the total distance travelled by the agent. The episode ends when the walker touches the ground or it reaches the far right side of the environment.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "### Formulation\n",
        "\n",
        "- *State* $s$:\n",
        "The state consists of 24 observations as follows:\n",
        "\n",
        "Num   | Observation                |  Min   |   Max  | Mean\n",
        "------|----------------------------|--------|--------|------   \n",
        "0     | hull_angle                 |  0     | 2$\\pi$ |  0.5\n",
        "1     | hull_angularVelocity       |  -inf  |  +inf  |  -\n",
        "2     | vel_x                      |  -1    |  +1    |  -\n",
        "3     |  vel_y                     |  -1    |  +1    |  -\n",
        "4     | hip_joint_1_angle          |  -inf  |  +inf  |  -\n",
        "5     | hip_joint_1_speed          |  -inf  |  +inf  |  -\n",
        "6     | knee_joint_1_angle         |  -inf  |  +inf  |  -\n",
        "7     | knee_joint_1_speed         |  -inf  |  +inf  |  -\n",
        "8     | leg_1_ground_contact_flag  |  0     |  1     |  -\n",
        "9     | hip_joint_2_angle          |  -inf  |  +inf  |  -\n",
        "10    | hip_joint_2_speed          |  -inf  |  +inf  |  -\n",
        "11    | knee_joint_2_angle         |  -inf  |  +inf  |  -\n",
        "12    | knee_joint_2_speed         |  -inf  |  +inf  |  -\n",
        "13    | leg_2_ground_contact_flag  |  0     |  1     |  -\n",
        "14-23 | 10 lidar readings          |  -inf  |  +inf  |  -\n",
        "\n",
        "\n",
        "- *Action $a$*:\n",
        "The continuous action has four dimensions:\n",
        "\n",
        "Num | Name                        | Min  | Max  \n",
        "----|-----------------------------|------|------\n",
        "0   | Hip_1 (Torque / Velocity)   |  -1  | +1\n",
        "1   | Knee_1 (Torque / Velocity)  |  -1  | +1\n",
        "2   | Hip_2 (Torque / Velocity)   |  -1  | +1\n",
        "3   | Knee_2 (Torque / Velocity)  |  -1  | +1\n",
        "\n",
        "\n",
        "- *Reward $r(s,a)$*:\n",
        "    \n",
        "    Reward is given for moving forward, the walker can get 300+ points when reaching the far end. If the walker falls, it gets -100.\n",
        "    \n",
        "\n",
        "- Episode Termination\n",
        "\n",
        "    An episode terminates when one of the following occurs:\n",
        "  - The walker reaches the far right side of the environment.\n",
        "  - The walker touches the ground.\n",
        "  - Episode length is greater than 1600.\n",
        "\n",
        "\n",
        "- *Objective*:\n",
        "    \n",
        "    To get an average reward of more than 270 over 50 consecutive trials (episodes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9CpC88gJ5fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ab3deb-cd34-488d-9d7a-d43c246ddb79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376096 sha256=e20384b99dbfdb0dfc04d8b2bfa53e8502ee18fd10c274b9e00c0eae30c08a97\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "# Import packages. Run this cell.\n",
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "questions",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "ywzptno4J5fV"
      },
      "source": [
        "### Task\n",
        "(50 points)\n",
        "\n",
        "We will use the Twin Delayed DDPG (TD3) algorithm to solve the \"BipedalWalker-v3\" task.\n",
        "\n",
        "Please complete the function ``train(self, cur_time_step, episode_time_step, state, action, reward, next_state, done)`` in the class ``TD3``. ``train(self, cur_time_step, episode_time_step, state, action, reward, next_state, done)`` will be called each step the agent interacts with the environment in the code provided for your training. The function collects samples and updates actor network and critic network using mini-batches of experience tuples. The input arguments are as follows:\n",
        "- cur_time_step: current time step counting from the beginning, which is equal to the number of times the agent interacts with the environment\n",
        "- episode_time_step: the time step counting from the current episode\n",
        "- state: current state, a numpy array with shape (state_size,)\n",
        "- action: current action, a numpy array with shape (action_size,)\n",
        "- reward: reward obtained\n",
        "- next_state: next state, a numpy array with shape (state_size,)\n",
        "- done: ``True`` when the current episode ends, ``False`` otherwise\n",
        "   \n",
        "You can also add or revise classes and functions if you need.\n",
        "\n",
        "After you complete the function, run the code to train your actor and critic networks, and save your actor network as ``actor.pth`` (save the model's ``state_dict``). Then upload ``actor.pth`` to Gradescope. We will test your policy (actor) for 50 episodes on Gradescope.\n",
        "\n",
        "**Note**:\n",
        "- You must use the class ``Actor`` as your actor network. Do not modify the structure of the actor network. Do not modify any code in the class ``Actor``.\n",
        "- You can use Google Colab to do the training.\n",
        "- You can use the class ``CriticQ`` as your critic network.\n",
        "- You can use GPU to accelerate the training, **but make sure that the actor network you saved is on CPU.**\n",
        "\n",
        "**Recommended Hyperparameters**:\n",
        "- Learning rate for the actor network: 1e-3\n",
        "- Learning rate for the critic network: 1e-3\n",
        "- Replay buffer capacity: 100000\n",
        "- Batch size: 128\n",
        "- Soft update step size for target networks: $\\tau$=0.02\n",
        "\n",
        "    Target networks are updated towards main networks according to:\n",
        "    \\begin{align}\n",
        "        \\theta_{\\text{targ}} \\leftarrow (1-\\tau) \\theta_{\\text{targ}} + \\tau \\theta\n",
        "    \\end{align}\n",
        "\n",
        "- Policy update period: 2\n",
        "\n",
        "    Policy will be updated once every 2 updates of the Q-networks.\n",
        "    \n",
        "- Discount factor: 0.99\n",
        "- Standard deviation for smoothing noise added to target policy: 0.2\n",
        "- Limit for absolute value of target policy smoothing noise: 0.5\n",
        "- Number of environment interactions that should elapse between gradient descent updates: 200\n",
        "\n",
        "    Note: Regardless of how long you wait between updates, the ratio of env steps to gradient steps is 1. For example, we update once every 200 environment interations and each update includes 200 gradient steps for Q-networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "question_code_networks",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "gv_g8egpJ5fW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Some parameters\n",
        "\"\"\"\n",
        "state_size = 24  # state dimension\n",
        "action_size = 4  # action dimension\n",
        "fc_units = 256  # number of neurons in one fully connected hidden layer\n",
        "action_upper_bound = 1  # action space upper bound\n",
        "action_lower_bound = -1  # action space lower bound\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Structure of Actor Network.\n",
        "\"\"\"\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_action = action_upper_bound\n",
        "        self.fc1 = nn.Linear(state_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, fc_units)\n",
        "        self.fc3 = nn.Linear(fc_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Build an actor (policy) network that maps states -> actions.\n",
        "        Args:\n",
        "            state: torch.Tensor with shape (batch_size, state_size)\n",
        "        Returns:\n",
        "            action: torch.Tensor with shape (batch_size, action_size)\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        action = torch.tanh(self.fc3(x)) * self.max_action\n",
        "        return action\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Structure of Critic Network.\n",
        "\"\"\"\n",
        "class CriticQ(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state_size: state dimension\n",
        "            action_size: action dimension\n",
        "            fc_units: number of neurons in one fully connected hidden layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Q-network 1 architecture\n",
        "        self.l1 = nn.Linear(state_size + action_size, fc_units)\n",
        "        self.l2 = nn.Linear(fc_units, fc_units)\n",
        "        self.l3 = nn.Linear(fc_units, 1)\n",
        "\n",
        "        # Q-network 2 architecture\n",
        "        self.l4 = nn.Linear(state_size + action_size, fc_units)\n",
        "        self.l5 = nn.Linear(fc_units, fc_units)\n",
        "        self.l6 = nn.Linear(fc_units, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"\n",
        "        Build a critic (value) network that maps state-action pairs -> Q-values.\n",
        "        Args:\n",
        "            state: torch.Tensor with shape (batch_size, state_size)\n",
        "            action: torch.Tensor with shape (batch_size, action_size)\n",
        "        Returns:\n",
        "            Q_value_1: torch.Tensor with shape (batch_size, 1)\n",
        "            Q_value_2: torch.Tensor with shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(state_action))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        Q_value_1 = self.l3(x1)\n",
        "\n",
        "        x2 = F.relu(self.l4(state_action))\n",
        "        x2 = F.relu(self.l5(x2))\n",
        "        Q_value_2 = self.l6(x2)\n",
        "\n",
        "        return Q_value_1, Q_value_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "question_code",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "N9ds4SfvJ5fW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of TD3 Algorithm\n",
        "\"\"\"\n",
        "class TD3:\n",
        "    def __init__(self):\n",
        "        self.lr_actor = 1e-3  # learning rate for actor network\n",
        "        self.lr_critic = 1e-3  # learning rate for critic network\n",
        "        self.buffer_capacity = 100000  # replay buffer capacity\n",
        "        self.batch_size = 128  # mini-batch size\n",
        "        self.tau = 0.02  # soft update parameter\n",
        "        self.policy_delay = 2  # policy will be updated once every policy_delay times for each update of the Q-networks.\n",
        "        self.gamma = 0.99  # discount factor\n",
        "        self.target_noise = 0.2  # standard deviation for smoothing noise added to target policy\n",
        "        self.noise_clip = 0.5  # limit for absolute value of target policy smoothing noise.\n",
        "        self.update_every = 200  # number of env interactions that should elapse between updates of Q-networks.\n",
        "        # Note: Regardless of how long you wait between updates, the ratio of env steps to gradient steps should be 1.\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = torch.device(\"cpu\")  # or self.device = torch.device(\"cuda\")\n",
        "        self.action_upper_bound = action_upper_bound  # action space upper bound\n",
        "        self.action_lower_bound = action_lower_bound  # action space lower bound\n",
        "        self.create_actor()\n",
        "        self.create_critic()\n",
        "        self.act_opt = optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
        "        self.crt_opt = optim.Adam(self.critic.parameters(), lr=self.lr_critic)\n",
        "        self.replay_memory_buffer = deque(maxlen=self.buffer_capacity)\n",
        "\n",
        "    def create_actor(self):\n",
        "        self.actor = Actor().to(self.device)\n",
        "        self.actor_target = Actor().to(self.device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "\n",
        "    def create_critic(self):\n",
        "        self.critic = CriticQ().to(self.device)\n",
        "        self.critic_target = CriticQ().to(self.device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add samples to replay memory\n",
        "        Args:\n",
        "            state: current state, a numpy array with shape (state_size,)\n",
        "            action: current action, a numpy array with shape (action_size,)\n",
        "            reward: reward obtained\n",
        "            next_state: next state, a numpy array with shape (state_size,)\n",
        "            done: True when the current episode ends, False otherwise\n",
        "        \"\"\"\n",
        "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def get_random_sample_from_replay_mem(self):\n",
        "        \"\"\"\n",
        "        Random samples from replay memory without replacement\n",
        "        Returns a self.batch_size length list of unique elements chosen from the replay buffer.\n",
        "        Returns:\n",
        "            random_sample: a list with len=self.batch_size,\n",
        "                           where each element is a tuple (state, action, reward, next_state, done)\n",
        "        \"\"\"\n",
        "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
        "        return random_sample\n",
        "\n",
        "    def soft_update_target(self, local_model, target_model):\n",
        "        \"\"\"\n",
        "        Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Args:\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
        "\n",
        "    def train(self, cur_time_step, episode_time_step, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Collect samples and update actor network and critic network using mini-batches of experience tuples.\n",
        "        Args:\n",
        "            cur_time_step: current time step counting from the beginning,\n",
        "                           which is equal to the number of times the agent interacts with the environment\n",
        "            episode_time_step: the time step counting from the current episode\n",
        "            state: current state, a numpy array with shape (state_size,)\n",
        "            action: current action, a numpy array with shape (action_size,)\n",
        "            reward: reward obtained\n",
        "            next_state: next state, a numpy array with shape (state_size,)\n",
        "            done: True when the current episode ends, False otherwise\n",
        "        \"\"\"\n",
        "        self.add_to_replay_memory(state, action, reward, next_state, done)\n",
        "        if len(self.replay_memory_buffer) < self.batch_size:\n",
        "            return\n",
        "        if cur_time_step % self.update_every != 0:\n",
        "            return\n",
        "\n",
        "        # Perform self.update_every times of updates of the critic networks and\n",
        "        # (self.update_every / policy_delay) times of updates of the actor network\n",
        "        for it in range(self.update_every):\n",
        "            \"\"\"\n",
        "            state_batch: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of current states\n",
        "            action_batch: torch.Tensor with shape (self.batch_size, action_size), a mini-batch of current actions\n",
        "            reward_batch: torch.Tensor with shape (self.batch_size, 1), a mini-batch of rewards\n",
        "            next_state_batch: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of next states\n",
        "            done_list: torch.Tensor with shape (self.batch_size, 1), a mini-batch of 0-1 integers,\n",
        "                   where 1 means the episode terminates for that sample;\n",
        "                         0 means the episode does not terminate for that sample.\n",
        "            \"\"\"\n",
        "            mini_batch = self.get_random_sample_from_replay_mem()\n",
        "            state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float().to(self.device)\n",
        "            action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).float().to(self.device)\n",
        "            reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float().to(self.device)\n",
        "            next_state_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float().to(self.device)\n",
        "            done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "            # Please complete codes for updating the critic networks\n",
        "            \"\"\"\n",
        "            Hints:\n",
        "              You may use the above tensors: state_batch, action_batch, reward_batch, next_state_batch, done_list\n",
        "              You may use self.critic_target and self.actor_target as your target networks\n",
        "              you may use target policy smoothing techniques with hyperparameters self.target_noise and self.noise_clip\n",
        "              You may use clipped double Q-learning techniques\n",
        "              You may update self.critic using the optimizer self.crt_opt and MSE loss function.\n",
        "              Make sure to consider whether the corresponding episode terminates when calculating target values.\n",
        "                If the episode terminates, then the next state value should be 0.\n",
        "            \"\"\"\n",
        "            ### BEGIN SOLUTION\n",
        "            # YOUR CODE HERE\n",
        "            if np.random.normal(0, self.target_noise**2) > self.noise_clip:\n",
        "                a = self.actor_target(next_state_batch) + self.noise_clip\n",
        "            elif np.random.normal(0, self.target_noise**2) < -self.noise_clip:\n",
        "                a = self.actor_target(next_state_batch) + -self.noise_clip\n",
        "            else:\n",
        "                a = self.actor_target(next_state_batch) + np.random.normal(0, self.target_noise**2)\n",
        "\n",
        "            possible_Q_values = self.critic_target(next_state_batch, a)\n",
        "            next_Q_value = torch.min(possible_Q_values[0], possible_Q_values[1])\n",
        "\n",
        "            target_Q = reward_batch + self.gamma * next_Q_value * (1-done_list)\n",
        "            current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n",
        "\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "            self.crt_opt.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.crt_opt.step()\n",
        "\n",
        "\n",
        "\n",
        "            #self.critic = np.min(sum((y - torch.min(Q_values[0], Q_values[1])) ** 2)/self.batch_size)\n",
        "\n",
        "            ### END SOLUTION\n",
        "\n",
        "            # Train Actor\n",
        "            # Delayed policy updates\n",
        "            # Update self.actor once every policy_delay times for each update of self.critic\n",
        "            if it % self.policy_delay == 0:\n",
        "\n",
        "                # Please complete codes for updating of the actor network\n",
        "                \"\"\"\n",
        "                Hint:\n",
        "                  You may update self.actor using the optimizer self.act_opt and recall the loss function for DDPG training\n",
        "                \"\"\"\n",
        "                ### BEGIN SOLUTION\n",
        "                # YOUR CODE HERE\n",
        "                pre_action = self.actor(state_batch)\n",
        "                Q1, Q2 = self.critic_target(state_batch, pre_action)\n",
        "                Q = torch.min(Q1, Q2)\n",
        "                actor_loss = - Q.mean()\n",
        "\n",
        "                # Optimize the actor\n",
        "                self.act_opt.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.act_opt.step()\n",
        "\n",
        "\n",
        "                #self.actor = (1/self.batch_size) * sum(-self.critic(state_batch, self.actor(state_batch))) * self.actor(state_batch)\n",
        "                ### END SOLUTION\n",
        "\n",
        "                # Soft update target models\n",
        "                self.soft_update_target(self.critic, self.critic_target)\n",
        "                self.soft_update_target(self.actor, self.actor_target)\n",
        "\n",
        "\n",
        "    def policy(self, state):\n",
        "        \"\"\"\n",
        "        Select action based on the actor network.\n",
        "        Args:\n",
        "            state: a numpy array with shape (state_size,)\n",
        "        Returns:\n",
        "            actions: a numpy array with shape (action_size,)\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            actions = np.squeeze(self.actor(state).cpu().data.numpy())\n",
        "        self.actor.train()\n",
        "        return actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "question_test_1",
          "locked": true,
          "points": 3,
          "schema_version": 1,
          "solution": false
        },
        "id": "RGcS7YsMJ5fY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a9513f-de3f-4632-f36b-b9db367cf3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep. 1, Ep.Timesteps 659, Episode Reward: -142.65\n",
            "Ep. 2, Ep.Timesteps 107, Episode Reward: -104.92\n",
            "Ep. 3, Ep.Timesteps 706, Episode Reward: -170.49\n",
            "Ep. 4, Ep.Timesteps 92, Episode Reward: -103.33\n",
            "Ep. 5, Ep.Timesteps 91, Episode Reward: -102.90\n",
            "Ep. 6, Ep.Timesteps 104, Episode Reward: -104.06\n",
            "Ep. 7, Ep.Timesteps 96, Episode Reward: -122.52\n",
            "Ep. 8, Ep.Timesteps 1600, Episode Reward: -173.73\n",
            "Ep. 9, Ep.Timesteps 65, Episode Reward: -108.24\n",
            "Ep. 10, Ep.Timesteps 97, Episode Reward: -108.70\n",
            "Ep. 11, Ep.Timesteps 69, Episode Reward: -121.17\n",
            "Ep. 12, Ep.Timesteps 69, Episode Reward: -120.95\n",
            "Ep. 13, Ep.Timesteps 68, Episode Reward: -121.95\n",
            "Ep. 14, Ep.Timesteps 52, Episode Reward: -106.69\n",
            "Ep. 15, Ep.Timesteps 51, Episode Reward: -107.00\n",
            "Ep. 16, Ep.Timesteps 51, Episode Reward: -106.90\n",
            "Ep. 17, Ep.Timesteps 47, Episode Reward: -107.67\n",
            "Ep. 18, Ep.Timesteps 50, Episode Reward: -102.49\n",
            "Ep. 19, Ep.Timesteps 50, Episode Reward: -102.54\n",
            "Ep. 20, Ep.Timesteps 50, Episode Reward: -102.53\n",
            "Ep. 21, Ep.Timesteps 50, Episode Reward: -102.51\n",
            "Ep. 22, Ep.Timesteps 52, Episode Reward: -101.70\n",
            "Ep. 23, Ep.Timesteps 52, Episode Reward: -101.70\n",
            "Ep. 24, Ep.Timesteps 52, Episode Reward: -101.70\n",
            "Ep. 25, Ep.Timesteps 52, Episode Reward: -101.81\n",
            "Ep. 26, Ep.Timesteps 55, Episode Reward: -103.35\n",
            "Ep. 27, Ep.Timesteps 55, Episode Reward: -103.36\n",
            "Ep. 28, Ep.Timesteps 55, Episode Reward: -103.52\n",
            "Ep. 29, Ep.Timesteps 46, Episode Reward: -105.55\n",
            "Ep. 30, Ep.Timesteps 49, Episode Reward: -104.75\n",
            "Ep. 31, Ep.Timesteps 50, Episode Reward: -104.24\n",
            "Ep. 32, Ep.Timesteps 50, Episode Reward: -104.38\n",
            "Ep. 33, Ep.Timesteps 49, Episode Reward: -104.61\n",
            "Ep. 34, Ep.Timesteps 54, Episode Reward: -107.40\n",
            "Ep. 35, Ep.Timesteps 55, Episode Reward: -107.80\n",
            "Ep. 36, Ep.Timesteps 55, Episode Reward: -107.67\n",
            "Ep. 37, Ep.Timesteps 65, Episode Reward: -102.67\n",
            "Ep. 38, Ep.Timesteps 65, Episode Reward: -102.66\n",
            "Ep. 39, Ep.Timesteps 65, Episode Reward: -102.66\n",
            "Ep. 40, Ep.Timesteps 58, Episode Reward: -102.22\n",
            "Ep. 41, Ep.Timesteps 42, Episode Reward: -105.13\n",
            "Ep. 42, Ep.Timesteps 47, Episode Reward: -102.98\n",
            "Ep. 43, Ep.Timesteps 46, Episode Reward: -102.75\n",
            "Ep. 44, Ep.Timesteps 45, Episode Reward: -102.69\n",
            "Ep. 45, Ep.Timesteps 47, Episode Reward: -103.17\n",
            "Ep. 46, Ep.Timesteps 49, Episode Reward: -104.65\n",
            "Ep. 47, Ep.Timesteps 47, Episode Reward: -103.17\n",
            "Ep. 48, Ep.Timesteps 48, Episode Reward: -102.82\n",
            "Ep. 49, Ep.Timesteps 49, Episode Reward: -102.19\n",
            "Ep. 50, Ep.Timesteps 49, Episode Reward: -102.18, Moving Average Reward: -108.99\n",
            "Ep. 51, Ep.Timesteps 49, Episode Reward: -102.18, Moving Average Reward: -108.18\n",
            "Ep. 52, Ep.Timesteps 49, Episode Reward: -102.59, Moving Average Reward: -108.13\n",
            "Ep. 53, Ep.Timesteps 47, Episode Reward: -102.71, Moving Average Reward: -106.78\n",
            "Ep. 54, Ep.Timesteps 52, Episode Reward: -101.19, Moving Average Reward: -106.73\n",
            "Ep. 55, Ep.Timesteps 52, Episode Reward: -103.26, Moving Average Reward: -106.74\n",
            "Ep. 56, Ep.Timesteps 51, Episode Reward: -105.65, Moving Average Reward: -106.77\n",
            "Ep. 57, Ep.Timesteps 59, Episode Reward: -106.82, Moving Average Reward: -106.46\n",
            "Ep. 58, Ep.Timesteps 59, Episode Reward: -106.83, Moving Average Reward: -105.12\n",
            "Ep. 59, Ep.Timesteps 59, Episode Reward: -106.62, Moving Average Reward: -105.09\n",
            "Ep. 60, Ep.Timesteps 56, Episode Reward: -104.78, Moving Average Reward: -105.01\n",
            "Ep. 61, Ep.Timesteps 56, Episode Reward: -105.05, Moving Average Reward: -104.69\n",
            "Ep. 62, Ep.Timesteps 56, Episode Reward: -105.65, Moving Average Reward: -104.38\n",
            "Ep. 63, Ep.Timesteps 57, Episode Reward: -108.00, Moving Average Reward: -104.10\n",
            "Ep. 64, Ep.Timesteps 95, Episode Reward: -106.03, Moving Average Reward: -104.09\n",
            "Ep. 65, Ep.Timesteps 95, Episode Reward: -107.29, Moving Average Reward: -104.09\n",
            "Ep. 66, Ep.Timesteps 93, Episode Reward: -121.16, Moving Average Reward: -104.38\n",
            "Ep. 67, Ep.Timesteps 92, Episode Reward: -120.45, Moving Average Reward: -104.64\n",
            "Ep. 68, Ep.Timesteps 68, Episode Reward: -105.80, Moving Average Reward: -104.70\n",
            "Ep. 69, Ep.Timesteps 68, Episode Reward: -105.75, Moving Average Reward: -104.77\n",
            "Ep. 70, Ep.Timesteps 64, Episode Reward: -105.03, Moving Average Reward: -104.82\n",
            "Ep. 71, Ep.Timesteps 72, Episode Reward: -103.03, Moving Average Reward: -104.83\n",
            "Ep. 72, Ep.Timesteps 56, Episode Reward: -105.35, Moving Average Reward: -104.90\n",
            "Ep. 73, Ep.Timesteps 56, Episode Reward: -105.34, Moving Average Reward: -104.97\n",
            "Ep. 74, Ep.Timesteps 77, Episode Reward: -101.29, Moving Average Reward: -104.96\n",
            "Ep. 75, Ep.Timesteps 63, Episode Reward: -98.81, Moving Average Reward: -104.90\n",
            "Ep. 76, Ep.Timesteps 63, Episode Reward: -99.16, Moving Average Reward: -104.82\n",
            "Ep. 77, Ep.Timesteps 70, Episode Reward: -102.55, Moving Average Reward: -104.80\n",
            "Ep. 78, Ep.Timesteps 78, Episode Reward: -102.16, Moving Average Reward: -104.78\n",
            "Ep. 79, Ep.Timesteps 76, Episode Reward: -101.79, Moving Average Reward: -104.70\n",
            "Ep. 80, Ep.Timesteps 1600, Episode Reward: -143.99, Moving Average Reward: -105.49\n",
            "Ep. 81, Ep.Timesteps 1600, Episode Reward: -139.97, Moving Average Reward: -106.20\n",
            "Ep. 82, Ep.Timesteps 53, Episode Reward: -99.47, Moving Average Reward: -106.10\n",
            "Ep. 83, Ep.Timesteps 57, Episode Reward: -100.49, Moving Average Reward: -106.02\n",
            "Ep. 84, Ep.Timesteps 1600, Episode Reward: -101.38, Moving Average Reward: -105.90\n",
            "Ep. 85, Ep.Timesteps 72, Episode Reward: -108.96, Moving Average Reward: -105.92\n",
            "Ep. 86, Ep.Timesteps 107, Episode Reward: -127.51, Moving Average Reward: -106.32\n",
            "Ep. 87, Ep.Timesteps 1600, Episode Reward: -129.80, Moving Average Reward: -106.86\n",
            "Ep. 88, Ep.Timesteps 90, Episode Reward: -100.29, Moving Average Reward: -106.82\n",
            "Ep. 89, Ep.Timesteps 135, Episode Reward: -128.12, Moving Average Reward: -107.32\n",
            "Ep. 90, Ep.Timesteps 81, Episode Reward: -102.63, Moving Average Reward: -107.33\n",
            "Ep. 91, Ep.Timesteps 148, Episode Reward: -100.89, Moving Average Reward: -107.25\n",
            "Ep. 92, Ep.Timesteps 102, Episode Reward: -103.18, Moving Average Reward: -107.25\n",
            "Ep. 93, Ep.Timesteps 106, Episode Reward: -102.62, Moving Average Reward: -107.25\n",
            "Ep. 94, Ep.Timesteps 109, Episode Reward: -100.79, Moving Average Reward: -107.21\n",
            "Ep. 95, Ep.Timesteps 84, Episode Reward: -101.94, Moving Average Reward: -107.19\n",
            "Ep. 96, Ep.Timesteps 92, Episode Reward: -100.40, Moving Average Reward: -107.10\n",
            "Ep. 97, Ep.Timesteps 72, Episode Reward: -101.45, Moving Average Reward: -107.07\n",
            "Ep. 98, Ep.Timesteps 86, Episode Reward: -101.96, Moving Average Reward: -107.05\n",
            "Ep. 99, Ep.Timesteps 100, Episode Reward: -104.75, Moving Average Reward: -107.10\n",
            "Ep. 100, Ep.Timesteps 81, Episode Reward: -105.37, Moving Average Reward: -107.17\n",
            "Ep. 101, Ep.Timesteps 77, Episode Reward: -105.18, Moving Average Reward: -107.23\n",
            "Ep. 102, Ep.Timesteps 48, Episode Reward: -105.86, Moving Average Reward: -107.29\n",
            "Ep. 103, Ep.Timesteps 48, Episode Reward: -105.83, Moving Average Reward: -107.35\n",
            "Ep. 104, Ep.Timesteps 48, Episode Reward: -105.88, Moving Average Reward: -107.45\n",
            "Ep. 105, Ep.Timesteps 142, Episode Reward: -100.92, Moving Average Reward: -107.40\n",
            "Ep. 106, Ep.Timesteps 114, Episode Reward: -101.22, Moving Average Reward: -107.31\n",
            "Ep. 107, Ep.Timesteps 148, Episode Reward: -133.29, Moving Average Reward: -107.84\n",
            "Ep. 108, Ep.Timesteps 84, Episode Reward: -126.19, Moving Average Reward: -108.23\n",
            "Ep. 109, Ep.Timesteps 126, Episode Reward: -137.72, Moving Average Reward: -108.85\n",
            "Ep. 110, Ep.Timesteps 101, Episode Reward: -125.71, Moving Average Reward: -109.27\n",
            "Ep. 111, Ep.Timesteps 107, Episode Reward: -126.42, Moving Average Reward: -109.70\n",
            "Ep. 112, Ep.Timesteps 97, Episode Reward: -128.77, Moving Average Reward: -110.16\n",
            "Ep. 113, Ep.Timesteps 95, Episode Reward: -127.02, Moving Average Reward: -110.54\n",
            "Ep. 114, Ep.Timesteps 107, Episode Reward: -103.93, Moving Average Reward: -110.50\n",
            "Ep. 115, Ep.Timesteps 1600, Episode Reward: -139.58, Moving Average Reward: -111.14\n",
            "Ep. 116, Ep.Timesteps 154, Episode Reward: -136.40, Moving Average Reward: -111.45\n",
            "Ep. 117, Ep.Timesteps 1600, Episode Reward: -156.66, Moving Average Reward: -112.17\n",
            "Ep. 118, Ep.Timesteps 1600, Episode Reward: -140.04, Moving Average Reward: -112.86\n",
            "Ep. 119, Ep.Timesteps 1600, Episode Reward: -130.09, Moving Average Reward: -113.34\n",
            "Ep. 120, Ep.Timesteps 144, Episode Reward: -143.15, Moving Average Reward: -114.11\n",
            "Ep. 121, Ep.Timesteps 1600, Episode Reward: -129.35, Moving Average Reward: -114.63\n",
            "Ep. 122, Ep.Timesteps 1600, Episode Reward: -149.53, Moving Average Reward: -115.52\n",
            "Ep. 123, Ep.Timesteps 1509, Episode Reward: -213.56, Moving Average Reward: -117.68\n",
            "Ep. 124, Ep.Timesteps 55, Episode Reward: -105.57, Moving Average Reward: -117.77\n",
            "Ep. 125, Ep.Timesteps 79, Episode Reward: -99.80, Moving Average Reward: -117.79\n",
            "Ep. 126, Ep.Timesteps 86, Episode Reward: -99.14, Moving Average Reward: -117.79\n",
            "Ep. 127, Ep.Timesteps 86, Episode Reward: -99.99, Moving Average Reward: -117.73\n",
            "Ep. 128, Ep.Timesteps 47, Episode Reward: -105.43, Moving Average Reward: -117.80\n",
            "Ep. 129, Ep.Timesteps 47, Episode Reward: -105.47, Moving Average Reward: -117.87\n",
            "Ep. 130, Ep.Timesteps 50, Episode Reward: -105.29, Moving Average Reward: -117.10\n",
            "Ep. 131, Ep.Timesteps 48, Episode Reward: -105.58, Moving Average Reward: -116.41\n",
            "Ep. 132, Ep.Timesteps 132, Episode Reward: -103.51, Moving Average Reward: -116.49\n",
            "Ep. 133, Ep.Timesteps 95, Episode Reward: -102.86, Moving Average Reward: -116.54\n",
            "Ep. 134, Ep.Timesteps 60, Episode Reward: -100.64, Moving Average Reward: -116.52\n",
            "Ep. 135, Ep.Timesteps 61, Episode Reward: -100.47, Moving Average Reward: -116.36\n",
            "Ep. 136, Ep.Timesteps 58, Episode Reward: -100.31, Moving Average Reward: -115.81\n",
            "Ep. 137, Ep.Timesteps 79, Episode Reward: -101.46, Moving Average Reward: -115.24\n",
            "Ep. 138, Ep.Timesteps 80, Episode Reward: -101.34, Moving Average Reward: -115.27\n",
            "Ep. 139, Ep.Timesteps 73, Episode Reward: -101.13, Moving Average Reward: -114.73\n",
            "Ep. 140, Ep.Timesteps 73, Episode Reward: -100.26, Moving Average Reward: -114.68\n",
            "Ep. 141, Ep.Timesteps 71, Episode Reward: -101.24, Moving Average Reward: -114.68\n",
            "Ep. 142, Ep.Timesteps 66, Episode Reward: -100.61, Moving Average Reward: -114.63\n",
            "Ep. 143, Ep.Timesteps 69, Episode Reward: -100.88, Moving Average Reward: -114.60\n",
            "Ep. 144, Ep.Timesteps 68, Episode Reward: -100.86, Moving Average Reward: -114.60\n",
            "Ep. 145, Ep.Timesteps 133, Episode Reward: -106.27, Moving Average Reward: -114.69\n",
            "Ep. 146, Ep.Timesteps 1600, Episode Reward: -131.12, Moving Average Reward: -115.30\n",
            "Ep. 147, Ep.Timesteps 81, Episode Reward: -98.99, Moving Average Reward: -115.25\n",
            "Ep. 148, Ep.Timesteps 1600, Episode Reward: -154.57, Moving Average Reward: -116.30\n",
            "Ep. 149, Ep.Timesteps 84, Episode Reward: -100.56, Moving Average Reward: -116.22\n",
            "Ep. 150, Ep.Timesteps 83, Episode Reward: -100.00, Moving Average Reward: -116.11\n",
            "Ep. 151, Ep.Timesteps 84, Episode Reward: -99.77, Moving Average Reward: -116.00\n",
            "Ep. 152, Ep.Timesteps 92, Episode Reward: -100.57, Moving Average Reward: -115.90\n",
            "Ep. 153, Ep.Timesteps 94, Episode Reward: -101.01, Moving Average Reward: -115.80\n",
            "Ep. 154, Ep.Timesteps 94, Episode Reward: -101.36, Moving Average Reward: -115.71\n",
            "Ep. 155, Ep.Timesteps 94, Episode Reward: -102.08, Moving Average Reward: -115.73\n",
            "Ep. 156, Ep.Timesteps 88, Episode Reward: -100.50, Moving Average Reward: -115.72\n",
            "Ep. 157, Ep.Timesteps 80, Episode Reward: -99.33, Moving Average Reward: -115.04\n",
            "Ep. 158, Ep.Timesteps 1600, Episode Reward: -120.39, Moving Average Reward: -114.93\n",
            "Ep. 159, Ep.Timesteps 1600, Episode Reward: -137.02, Moving Average Reward: -114.91\n",
            "Ep. 160, Ep.Timesteps 1600, Episode Reward: -178.02, Moving Average Reward: -115.96\n",
            "Ep. 161, Ep.Timesteps 1600, Episode Reward: -130.35, Moving Average Reward: -116.04\n",
            "Ep. 162, Ep.Timesteps 1600, Episode Reward: -117.41, Moving Average Reward: -115.81\n",
            "Ep. 163, Ep.Timesteps 137, Episode Reward: -107.77, Moving Average Reward: -115.42\n",
            "Ep. 164, Ep.Timesteps 78, Episode Reward: -124.92, Moving Average Reward: -115.84\n",
            "Ep. 165, Ep.Timesteps 1600, Episode Reward: -155.66, Moving Average Reward: -116.17\n",
            "Ep. 166, Ep.Timesteps 1600, Episode Reward: -82.18, Moving Average Reward: -115.08\n",
            "Ep. 167, Ep.Timesteps 1600, Episode Reward: -131.93, Moving Average Reward: -114.59\n",
            "Ep. 168, Ep.Timesteps 1600, Episode Reward: -104.30, Moving Average Reward: -113.87\n",
            "Ep. 169, Ep.Timesteps 1600, Episode Reward: -90.31, Moving Average Reward: -113.08\n",
            "Ep. 170, Ep.Timesteps 295, Episode Reward: -106.83, Moving Average Reward: -112.35\n",
            "Ep. 171, Ep.Timesteps 98, Episode Reward: -98.57, Moving Average Reward: -111.73\n",
            "Ep. 172, Ep.Timesteps 97, Episode Reward: -105.32, Moving Average Reward: -110.85\n",
            "Ep. 173, Ep.Timesteps 1600, Episode Reward: -98.11, Moving Average Reward: -108.54\n",
            "Ep. 174, Ep.Timesteps 103, Episode Reward: -112.82, Moving Average Reward: -108.69\n",
            "Ep. 175, Ep.Timesteps 231, Episode Reward: -122.63, Moving Average Reward: -109.14\n",
            "Ep. 176, Ep.Timesteps 108, Episode Reward: -108.89, Moving Average Reward: -109.34\n",
            "Ep. 177, Ep.Timesteps 108, Episode Reward: -109.64, Moving Average Reward: -109.53\n",
            "Ep. 178, Ep.Timesteps 82, Episode Reward: -110.07, Moving Average Reward: -109.62\n",
            "Ep. 179, Ep.Timesteps 94, Episode Reward: -110.73, Moving Average Reward: -109.73\n",
            "Ep. 180, Ep.Timesteps 93, Episode Reward: -111.57, Moving Average Reward: -109.85\n",
            "Ep. 181, Ep.Timesteps 116, Episode Reward: -108.90, Moving Average Reward: -109.92\n",
            "Ep. 182, Ep.Timesteps 115, Episode Reward: -109.41, Moving Average Reward: -110.04\n",
            "Ep. 183, Ep.Timesteps 1600, Episode Reward: -62.09, Moving Average Reward: -109.22\n",
            "Ep. 184, Ep.Timesteps 211, Episode Reward: -107.19, Moving Average Reward: -109.35\n",
            "Ep. 185, Ep.Timesteps 1600, Episode Reward: -70.21, Moving Average Reward: -108.75\n",
            "Ep. 186, Ep.Timesteps 1600, Episode Reward: -87.76, Moving Average Reward: -108.50\n",
            "Ep. 187, Ep.Timesteps 1600, Episode Reward: -84.37, Moving Average Reward: -108.16\n",
            "Ep. 188, Ep.Timesteps 1600, Episode Reward: -60.86, Moving Average Reward: -107.35\n",
            "Ep. 189, Ep.Timesteps 1600, Episode Reward: -79.15, Moving Average Reward: -106.91\n",
            "Ep. 190, Ep.Timesteps 1600, Episode Reward: -77.29, Moving Average Reward: -106.45\n",
            "Ep. 191, Ep.Timesteps 1600, Episode Reward: -139.44, Moving Average Reward: -107.21\n",
            "Ep. 192, Ep.Timesteps 185, Episode Reward: -110.69, Moving Average Reward: -107.41\n",
            "Ep. 193, Ep.Timesteps 1600, Episode Reward: -87.26, Moving Average Reward: -107.14\n",
            "Ep. 194, Ep.Timesteps 100, Episode Reward: -127.21, Moving Average Reward: -107.67\n",
            "Ep. 195, Ep.Timesteps 1600, Episode Reward: -101.72, Moving Average Reward: -107.58\n",
            "Ep. 196, Ep.Timesteps 1600, Episode Reward: -70.90, Moving Average Reward: -106.37\n",
            "Ep. 197, Ep.Timesteps 1600, Episode Reward: -138.01, Moving Average Reward: -107.15\n",
            "Ep. 198, Ep.Timesteps 1600, Episode Reward: -85.14, Moving Average Reward: -105.76\n",
            "Ep. 199, Ep.Timesteps 69, Episode Reward: -123.58, Moving Average Reward: -106.22\n",
            "Ep. 200, Ep.Timesteps 61, Episode Reward: -115.32, Moving Average Reward: -106.53\n",
            "Ep. 201, Ep.Timesteps 1600, Episode Reward: -134.56, Moving Average Reward: -107.23\n",
            "Ep. 202, Ep.Timesteps 137, Episode Reward: -99.95, Moving Average Reward: -107.21\n",
            "Ep. 203, Ep.Timesteps 1600, Episode Reward: -81.93, Moving Average Reward: -106.83\n",
            "Ep. 204, Ep.Timesteps 118, Episode Reward: -102.54, Moving Average Reward: -106.86\n",
            "Ep. 205, Ep.Timesteps 145, Episode Reward: -106.07, Moving Average Reward: -106.94\n",
            "Ep. 206, Ep.Timesteps 83, Episode Reward: -104.39, Moving Average Reward: -107.01\n",
            "Ep. 207, Ep.Timesteps 60, Episode Reward: -109.18, Moving Average Reward: -107.21\n",
            "Ep. 208, Ep.Timesteps 81, Episode Reward: -107.26, Moving Average Reward: -106.95\n",
            "Ep. 209, Ep.Timesteps 76, Episode Reward: -108.25, Moving Average Reward: -106.37\n",
            "Ep. 210, Ep.Timesteps 100, Episode Reward: -107.43, Moving Average Reward: -104.96\n",
            "Ep. 211, Ep.Timesteps 84, Episode Reward: -108.21, Moving Average Reward: -104.52\n",
            "Ep. 212, Ep.Timesteps 87, Episode Reward: -107.50, Moving Average Reward: -104.32\n",
            "Ep. 213, Ep.Timesteps 113, Episode Reward: -114.75, Moving Average Reward: -104.46\n",
            "Ep. 214, Ep.Timesteps 1600, Episode Reward: -55.40, Moving Average Reward: -103.07\n",
            "Ep. 215, Ep.Timesteps 1600, Episode Reward: -48.55, Moving Average Reward: -100.93\n",
            "Ep. 216, Ep.Timesteps 1600, Episode Reward: -111.46, Moving Average Reward: -101.51\n",
            "Ep. 217, Ep.Timesteps 1600, Episode Reward: -136.76, Moving Average Reward: -101.61\n",
            "Ep. 218, Ep.Timesteps 1600, Episode Reward: -103.50, Moving Average Reward: -101.59\n",
            "Ep. 219, Ep.Timesteps 85, Episode Reward: -107.60, Moving Average Reward: -101.94\n",
            "Ep. 220, Ep.Timesteps 251, Episode Reward: -147.24, Moving Average Reward: -102.75\n",
            "Ep. 221, Ep.Timesteps 82, Episode Reward: -125.46, Moving Average Reward: -103.28\n",
            "Ep. 222, Ep.Timesteps 79, Episode Reward: -124.97, Moving Average Reward: -103.68\n",
            "Ep. 223, Ep.Timesteps 81, Episode Reward: -125.96, Moving Average Reward: -104.23\n",
            "Ep. 224, Ep.Timesteps 1600, Episode Reward: -150.23, Moving Average Reward: -104.98\n",
            "Ep. 225, Ep.Timesteps 1600, Episode Reward: -107.43, Moving Average Reward: -104.68\n",
            "Ep. 226, Ep.Timesteps 1600, Episode Reward: -101.99, Moving Average Reward: -104.54\n",
            "Ep. 227, Ep.Timesteps 59, Episode Reward: -109.86, Moving Average Reward: -104.55\n",
            "Ep. 228, Ep.Timesteps 59, Episode Reward: -109.46, Moving Average Reward: -104.53\n",
            "Ep. 229, Ep.Timesteps 74, Episode Reward: -111.41, Moving Average Reward: -104.55\n",
            "Ep. 230, Ep.Timesteps 63, Episode Reward: -110.25, Moving Average Reward: -104.52\n",
            "Ep. 231, Ep.Timesteps 63, Episode Reward: -110.26, Moving Average Reward: -104.55\n",
            "Ep. 232, Ep.Timesteps 77, Episode Reward: -107.23, Moving Average Reward: -104.50\n",
            "Ep. 233, Ep.Timesteps 62, Episode Reward: -102.54, Moving Average Reward: -105.31\n",
            "Ep. 234, Ep.Timesteps 55, Episode Reward: -104.24, Moving Average Reward: -105.25\n",
            "Ep. 235, Ep.Timesteps 63, Episode Reward: -105.38, Moving Average Reward: -105.96\n",
            "Ep. 236, Ep.Timesteps 69, Episode Reward: -109.90, Moving Average Reward: -106.40\n",
            "Ep. 237, Ep.Timesteps 70, Episode Reward: -110.07, Moving Average Reward: -106.91\n",
            "Ep. 238, Ep.Timesteps 139, Episode Reward: -140.40, Moving Average Reward: -108.51\n",
            "Ep. 239, Ep.Timesteps 1600, Episode Reward: -68.42, Moving Average Reward: -108.29\n",
            "Ep. 240, Ep.Timesteps 1600, Episode Reward: -63.77, Moving Average Reward: -108.02\n",
            "Ep. 241, Ep.Timesteps 1600, Episode Reward: -72.34, Moving Average Reward: -106.68\n",
            "Ep. 242, Ep.Timesteps 1600, Episode Reward: -65.82, Moving Average Reward: -105.78\n",
            "Ep. 243, Ep.Timesteps 147, Episode Reward: -140.03, Moving Average Reward: -106.84\n",
            "Ep. 244, Ep.Timesteps 1600, Episode Reward: -73.58, Moving Average Reward: -105.76\n",
            "Ep. 245, Ep.Timesteps 166, Episode Reward: -128.24, Moving Average Reward: -106.29\n",
            "Ep. 246, Ep.Timesteps 69, Episode Reward: -111.50, Moving Average Reward: -107.11\n",
            "Ep. 247, Ep.Timesteps 63, Episode Reward: -109.93, Moving Average Reward: -106.54\n",
            "Ep. 248, Ep.Timesteps 67, Episode Reward: -102.40, Moving Average Reward: -106.89\n",
            "Ep. 249, Ep.Timesteps 1600, Episode Reward: -51.09, Moving Average Reward: -105.44\n",
            "Ep. 250, Ep.Timesteps 1600, Episode Reward: -60.66, Moving Average Reward: -104.35\n",
            "Ep. 251, Ep.Timesteps 1600, Episode Reward: -54.52, Moving Average Reward: -102.75\n",
            "Ep. 252, Ep.Timesteps 1600, Episode Reward: -70.22, Moving Average Reward: -102.15\n",
            "Ep. 253, Ep.Timesteps 1600, Episode Reward: -57.52, Moving Average Reward: -101.66\n",
            "Ep. 254, Ep.Timesteps 100, Episode Reward: -100.42, Moving Average Reward: -101.62\n",
            "Ep. 255, Ep.Timesteps 1600, Episode Reward: -51.97, Moving Average Reward: -100.54\n",
            "Ep. 256, Ep.Timesteps 1600, Episode Reward: -44.20, Moving Average Reward: -99.33\n",
            "Ep. 257, Ep.Timesteps 1600, Episode Reward: -58.03, Moving Average Reward: -98.31\n",
            "Ep. 258, Ep.Timesteps 1600, Episode Reward: -53.27, Moving Average Reward: -97.23\n",
            "Ep. 259, Ep.Timesteps 1600, Episode Reward: -59.90, Moving Average Reward: -96.27\n",
            "Ep. 260, Ep.Timesteps 1600, Episode Reward: -64.01, Moving Average Reward: -95.40\n",
            "Ep. 261, Ep.Timesteps 1600, Episode Reward: -68.91, Moving Average Reward: -94.61\n",
            "Ep. 262, Ep.Timesteps 1600, Episode Reward: -92.40, Moving Average Reward: -94.31\n",
            "Ep. 263, Ep.Timesteps 1600, Episode Reward: -61.90, Moving Average Reward: -93.25\n",
            "Ep. 264, Ep.Timesteps 1600, Episode Reward: -60.95, Moving Average Reward: -93.36\n",
            "Ep. 265, Ep.Timesteps 80, Episode Reward: -125.18, Moving Average Reward: -94.90\n",
            "Ep. 266, Ep.Timesteps 1600, Episode Reward: -42.50, Moving Average Reward: -93.52\n",
            "Ep. 267, Ep.Timesteps 1600, Episode Reward: -75.43, Moving Average Reward: -92.29\n",
            "Ep. 268, Ep.Timesteps 1600, Episode Reward: -66.40, Moving Average Reward: -91.55\n",
            "Ep. 269, Ep.Timesteps 1600, Episode Reward: -71.93, Moving Average Reward: -90.83\n",
            "Ep. 270, Ep.Timesteps 1600, Episode Reward: -57.18, Moving Average Reward: -89.03\n",
            "Ep. 271, Ep.Timesteps 1600, Episode Reward: -51.16, Moving Average Reward: -87.55\n",
            "Ep. 272, Ep.Timesteps 1600, Episode Reward: -72.84, Moving Average Reward: -86.50\n",
            "Ep. 273, Ep.Timesteps 1600, Episode Reward: -88.64, Moving Average Reward: -85.76\n",
            "Ep. 274, Ep.Timesteps 1600, Episode Reward: -69.07, Moving Average Reward: -84.13\n",
            "Ep. 275, Ep.Timesteps 1009, Episode Reward: -132.94, Moving Average Reward: -84.65\n",
            "Ep. 276, Ep.Timesteps 1600, Episode Reward: -48.61, Moving Average Reward: -83.58\n",
            "Ep. 277, Ep.Timesteps 1600, Episode Reward: -73.94, Moving Average Reward: -82.86\n",
            "Ep. 278, Ep.Timesteps 1600, Episode Reward: -57.93, Moving Average Reward: -81.83\n",
            "Ep. 279, Ep.Timesteps 117, Episode Reward: -105.99, Moving Average Reward: -81.72\n",
            "Ep. 280, Ep.Timesteps 85, Episode Reward: -117.93, Moving Average Reward: -81.87\n",
            "Ep. 281, Ep.Timesteps 75, Episode Reward: -116.76, Moving Average Reward: -82.00\n",
            "Ep. 282, Ep.Timesteps 71, Episode Reward: -123.86, Moving Average Reward: -82.34\n",
            "Ep. 283, Ep.Timesteps 1600, Episode Reward: -115.76, Moving Average Reward: -82.60\n",
            "Ep. 284, Ep.Timesteps 1600, Episode Reward: -44.60, Moving Average Reward: -81.41\n",
            "Ep. 285, Ep.Timesteps 1600, Episode Reward: -65.62, Moving Average Reward: -80.61\n",
            "Ep. 286, Ep.Timesteps 1600, Episode Reward: -71.19, Moving Average Reward: -79.84\n",
            "Ep. 287, Ep.Timesteps 158, Episode Reward: -106.87, Moving Average Reward: -79.77\n",
            "Ep. 288, Ep.Timesteps 94, Episode Reward: -99.31, Moving Average Reward: -78.95\n",
            "Ep. 289, Ep.Timesteps 81, Episode Reward: -102.03, Moving Average Reward: -79.62\n",
            "Ep. 290, Ep.Timesteps 110, Episode Reward: -98.51, Moving Average Reward: -80.32\n",
            "Ep. 291, Ep.Timesteps 168, Episode Reward: -100.22, Moving Average Reward: -80.88\n",
            "Ep. 292, Ep.Timesteps 101, Episode Reward: -100.17, Moving Average Reward: -81.56\n",
            "Ep. 293, Ep.Timesteps 1600, Episode Reward: -103.03, Moving Average Reward: -80.82\n",
            "Ep. 294, Ep.Timesteps 1600, Episode Reward: -101.73, Moving Average Reward: -81.39\n",
            "Ep. 295, Ep.Timesteps 1600, Episode Reward: -99.78, Moving Average Reward: -80.82\n",
            "Ep. 296, Ep.Timesteps 1600, Episode Reward: -66.04, Moving Average Reward: -79.91\n",
            "Ep. 297, Ep.Timesteps 1600, Episode Reward: -98.26, Moving Average Reward: -79.68\n",
            "Ep. 298, Ep.Timesteps 1600, Episode Reward: -88.43, Moving Average Reward: -79.40\n",
            "Ep. 299, Ep.Timesteps 180, Episode Reward: -104.77, Moving Average Reward: -80.47\n",
            "Ep. 300, Ep.Timesteps 1600, Episode Reward: -99.66, Moving Average Reward: -81.25\n",
            "Ep. 301, Ep.Timesteps 1600, Episode Reward: -100.61, Moving Average Reward: -82.17\n",
            "Ep. 302, Ep.Timesteps 1600, Episode Reward: -113.27, Moving Average Reward: -83.03\n",
            "Ep. 303, Ep.Timesteps 168, Episode Reward: -106.16, Moving Average Reward: -84.01\n",
            "Ep. 304, Ep.Timesteps 1600, Episode Reward: -92.78, Moving Average Reward: -83.85\n",
            "Ep. 305, Ep.Timesteps 1600, Episode Reward: -72.72, Moving Average Reward: -84.27\n",
            "Ep. 306, Ep.Timesteps 1600, Episode Reward: -76.98, Moving Average Reward: -84.92\n",
            "Ep. 307, Ep.Timesteps 1600, Episode Reward: -71.14, Moving Average Reward: -85.19\n",
            "Ep. 308, Ep.Timesteps 1600, Episode Reward: -76.69, Moving Average Reward: -85.65\n",
            "Ep. 309, Ep.Timesteps 1600, Episode Reward: -52.77, Moving Average Reward: -85.51\n",
            "Ep. 310, Ep.Timesteps 1421, Episode Reward: -179.41, Moving Average Reward: -87.82\n",
            "Ep. 311, Ep.Timesteps 1600, Episode Reward: -56.77, Moving Average Reward: -87.58\n",
            "Ep. 312, Ep.Timesteps 1600, Episode Reward: -159.45, Moving Average Reward: -88.92\n",
            "Ep. 313, Ep.Timesteps 1600, Episode Reward: -96.16, Moving Average Reward: -89.60\n",
            "Ep. 314, Ep.Timesteps 1600, Episode Reward: -54.20, Moving Average Reward: -89.47\n",
            "Ep. 315, Ep.Timesteps 1600, Episode Reward: -53.15, Moving Average Reward: -88.03\n",
            "Ep. 316, Ep.Timesteps 279, Episode Reward: -110.00, Moving Average Reward: -89.38\n",
            "Ep. 317, Ep.Timesteps 1600, Episode Reward: -103.81, Moving Average Reward: -89.94\n",
            "Ep. 318, Ep.Timesteps 1600, Episode Reward: -84.87, Moving Average Reward: -90.31\n",
            "Ep. 319, Ep.Timesteps 1600, Episode Reward: -103.64, Moving Average Reward: -90.95\n",
            "Ep. 320, Ep.Timesteps 1600, Episode Reward: -56.78, Moving Average Reward: -90.94\n",
            "Ep. 321, Ep.Timesteps 1600, Episode Reward: -55.33, Moving Average Reward: -91.02\n",
            "Ep. 322, Ep.Timesteps 1600, Episode Reward: -122.16, Moving Average Reward: -92.01\n",
            "Ep. 323, Ep.Timesteps 1600, Episode Reward: -40.68, Moving Average Reward: -91.05\n",
            "Ep. 324, Ep.Timesteps 1600, Episode Reward: -57.14, Moving Average Reward: -90.81\n",
            "Ep. 325, Ep.Timesteps 1600, Episode Reward: -47.96, Moving Average Reward: -89.11\n",
            "Ep. 326, Ep.Timesteps 809, Episode Reward: -144.44, Moving Average Reward: -91.03\n",
            "Ep. 327, Ep.Timesteps 71, Episode Reward: -114.35, Moving Average Reward: -91.84\n",
            "Ep. 328, Ep.Timesteps 43, Episode Reward: -107.72, Moving Average Reward: -92.83\n",
            "Ep. 329, Ep.Timesteps 47, Episode Reward: -109.21, Moving Average Reward: -92.90\n",
            "Ep. 330, Ep.Timesteps 58, Episode Reward: -111.67, Moving Average Reward: -92.77\n",
            "Ep. 331, Ep.Timesteps 64, Episode Reward: -113.29, Moving Average Reward: -92.70\n",
            "Ep. 332, Ep.Timesteps 44, Episode Reward: -110.43, Moving Average Reward: -92.43\n",
            "Ep. 333, Ep.Timesteps 44, Episode Reward: -110.39, Moving Average Reward: -92.33\n",
            "Ep. 334, Ep.Timesteps 44, Episode Reward: -110.39, Moving Average Reward: -93.64\n",
            "Ep. 335, Ep.Timesteps 44, Episode Reward: -110.48, Moving Average Reward: -94.54\n",
            "Ep. 336, Ep.Timesteps 45, Episode Reward: -109.70, Moving Average Reward: -95.31\n",
            "Ep. 337, Ep.Timesteps 1600, Episode Reward: -79.25, Moving Average Reward: -94.76\n",
            "Ep. 338, Ep.Timesteps 140, Episode Reward: -110.90, Moving Average Reward: -94.99\n",
            "Ep. 339, Ep.Timesteps 145, Episode Reward: -110.94, Moving Average Reward: -95.17\n",
            "Ep. 340, Ep.Timesteps 1600, Episode Reward: -47.77, Moving Average Reward: -94.15\n",
            "Ep. 341, Ep.Timesteps 1600, Episode Reward: -52.36, Moving Average Reward: -93.20\n",
            "Ep. 342, Ep.Timesteps 1600, Episode Reward: -48.93, Moving Average Reward: -92.17\n",
            "Ep. 343, Ep.Timesteps 1600, Episode Reward: -47.89, Moving Average Reward: -91.07\n",
            "Ep. 344, Ep.Timesteps 1600, Episode Reward: -51.01, Moving Average Reward: -90.05\n",
            "Ep. 345, Ep.Timesteps 1600, Episode Reward: -45.61, Moving Average Reward: -88.97\n",
            "Ep. 346, Ep.Timesteps 1600, Episode Reward: -71.73, Moving Average Reward: -89.08\n",
            "Ep. 347, Ep.Timesteps 208, Episode Reward: -103.15, Moving Average Reward: -89.18\n",
            "Ep. 348, Ep.Timesteps 215, Episode Reward: -101.15, Moving Average Reward: -89.44\n",
            "Ep. 349, Ep.Timesteps 252, Episode Reward: -104.30, Moving Average Reward: -89.43\n",
            "Ep. 350, Ep.Timesteps 897, Episode Reward: -134.18, Moving Average Reward: -90.12\n",
            "Ep. 351, Ep.Timesteps 231, Episode Reward: -105.33, Moving Average Reward: -90.21\n",
            "Ep. 352, Ep.Timesteps 1600, Episode Reward: -85.82, Moving Average Reward: -89.66\n",
            "Ep. 353, Ep.Timesteps 1600, Episode Reward: -73.51, Moving Average Reward: -89.01\n",
            "Ep. 354, Ep.Timesteps 1600, Episode Reward: -54.51, Moving Average Reward: -88.24\n",
            "Ep. 355, Ep.Timesteps 1600, Episode Reward: -52.47, Moving Average Reward: -87.84\n",
            "Ep. 356, Ep.Timesteps 1600, Episode Reward: -40.79, Moving Average Reward: -87.12\n",
            "Ep. 357, Ep.Timesteps 1600, Episode Reward: -50.81, Moving Average Reward: -86.71\n",
            "Ep. 358, Ep.Timesteps 1600, Episode Reward: -132.24, Moving Average Reward: -87.82\n",
            "Ep. 359, Ep.Timesteps 93, Episode Reward: -111.19, Moving Average Reward: -88.99\n",
            "Ep. 360, Ep.Timesteps 1600, Episode Reward: -42.22, Moving Average Reward: -86.24\n",
            "Ep. 361, Ep.Timesteps 1600, Episode Reward: -81.30, Moving Average Reward: -86.74\n",
            "Ep. 362, Ep.Timesteps 1600, Episode Reward: -50.10, Moving Average Reward: -84.55\n",
            "Ep. 363, Ep.Timesteps 1600, Episode Reward: -49.46, Moving Average Reward: -83.61\n",
            "Ep. 364, Ep.Timesteps 1600, Episode Reward: -34.94, Moving Average Reward: -83.23\n",
            "Ep. 365, Ep.Timesteps 1600, Episode Reward: -92.76, Moving Average Reward: -84.02\n",
            "Ep. 366, Ep.Timesteps 1600, Episode Reward: -116.10, Moving Average Reward: -84.14\n",
            "Ep. 367, Ep.Timesteps 1600, Episode Reward: -59.08, Moving Average Reward: -83.25\n",
            "Ep. 368, Ep.Timesteps 1600, Episode Reward: -37.14, Moving Average Reward: -82.29\n",
            "Ep. 369, Ep.Timesteps 1600, Episode Reward: -36.45, Moving Average Reward: -80.95\n",
            "Ep. 370, Ep.Timesteps 1600, Episode Reward: -23.59, Moving Average Reward: -80.29\n",
            "Ep. 371, Ep.Timesteps 1600, Episode Reward: -46.53, Moving Average Reward: -80.11\n",
            "Ep. 372, Ep.Timesteps 1600, Episode Reward: -111.01, Moving Average Reward: -79.89\n",
            "Ep. 373, Ep.Timesteps 1600, Episode Reward: -73.96, Moving Average Reward: -80.55\n",
            "Ep. 374, Ep.Timesteps 322, Episode Reward: -109.61, Moving Average Reward: -81.60\n",
            "Ep. 375, Ep.Timesteps 1600, Episode Reward: -17.82, Moving Average Reward: -81.00\n",
            "Ep. 376, Ep.Timesteps 1600, Episode Reward: -56.62, Moving Average Reward: -79.24\n",
            "Ep. 377, Ep.Timesteps 362, Episode Reward: -106.16, Moving Average Reward: -79.08\n",
            "Ep. 378, Ep.Timesteps 183, Episode Reward: -86.13, Moving Average Reward: -78.65\n",
            "Ep. 379, Ep.Timesteps 106, Episode Reward: -93.52, Moving Average Reward: -78.33\n",
            "Ep. 380, Ep.Timesteps 946, Episode Reward: -119.70, Moving Average Reward: -78.49\n",
            "Ep. 381, Ep.Timesteps 1600, Episode Reward: -92.96, Moving Average Reward: -78.09\n",
            "Ep. 382, Ep.Timesteps 1600, Episode Reward: -83.29, Moving Average Reward: -77.54\n",
            "Ep. 383, Ep.Timesteps 1600, Episode Reward: -56.99, Moving Average Reward: -76.48\n",
            "Ep. 384, Ep.Timesteps 1600, Episode Reward: -67.03, Moving Average Reward: -75.61\n",
            "Ep. 385, Ep.Timesteps 1600, Episode Reward: -44.53, Moving Average Reward: -74.29\n",
            "Ep. 386, Ep.Timesteps 1600, Episode Reward: -41.25, Moving Average Reward: -72.92\n",
            "Ep. 387, Ep.Timesteps 158, Episode Reward: -95.32, Moving Average Reward: -73.24\n",
            "Ep. 388, Ep.Timesteps 1600, Episode Reward: -68.10, Moving Average Reward: -72.39\n",
            "Ep. 389, Ep.Timesteps 1600, Episode Reward: -99.03, Moving Average Reward: -72.15\n",
            "Ep. 390, Ep.Timesteps 1600, Episode Reward: -65.50, Moving Average Reward: -72.50\n",
            "Ep. 391, Ep.Timesteps 1600, Episode Reward: -48.70, Moving Average Reward: -72.43\n",
            "Ep. 392, Ep.Timesteps 1600, Episode Reward: -48.77, Moving Average Reward: -72.43\n",
            "Ep. 393, Ep.Timesteps 1600, Episode Reward: -62.76, Moving Average Reward: -72.72\n",
            "Ep. 394, Ep.Timesteps 1600, Episode Reward: -45.43, Moving Average Reward: -72.61\n",
            "Ep. 395, Ep.Timesteps 1600, Episode Reward: -67.31, Moving Average Reward: -73.05\n",
            "Ep. 396, Ep.Timesteps 1600, Episode Reward: -63.11, Moving Average Reward: -72.87\n",
            "Ep. 397, Ep.Timesteps 930, Episode Reward: -130.53, Moving Average Reward: -73.42\n",
            "Ep. 398, Ep.Timesteps 1600, Episode Reward: -57.46, Moving Average Reward: -72.55\n",
            "Ep. 399, Ep.Timesteps 1263, Episode Reward: -166.24, Moving Average Reward: -73.79\n",
            "Ep. 400, Ep.Timesteps 1600, Episode Reward: -41.28, Moving Average Reward: -71.93\n",
            "Ep. 401, Ep.Timesteps 1600, Episode Reward: -45.27, Moving Average Reward: -70.73\n",
            "Ep. 402, Ep.Timesteps 1600, Episode Reward: -56.23, Moving Average Reward: -70.14\n",
            "Ep. 403, Ep.Timesteps 1600, Episode Reward: -71.89, Moving Average Reward: -70.10\n",
            "Ep. 404, Ep.Timesteps 1600, Episode Reward: -94.47, Moving Average Reward: -70.90\n",
            "Ep. 405, Ep.Timesteps 1600, Episode Reward: -30.66, Moving Average Reward: -70.47\n",
            "Ep. 406, Ep.Timesteps 1600, Episode Reward: -40.35, Moving Average Reward: -70.46\n",
            "Ep. 407, Ep.Timesteps 1600, Episode Reward: -85.15, Moving Average Reward: -71.14\n",
            "Ep. 408, Ep.Timesteps 1600, Episode Reward: -38.41, Moving Average Reward: -69.27\n",
            "Ep. 409, Ep.Timesteps 190, Episode Reward: -108.06, Moving Average Reward: -69.21\n",
            "Ep. 410, Ep.Timesteps 1600, Episode Reward: -15.79, Moving Average Reward: -68.68\n",
            "Ep. 411, Ep.Timesteps 1600, Episode Reward: -91.91, Moving Average Reward: -68.89\n",
            "Ep. 412, Ep.Timesteps 99, Episode Reward: -126.47, Moving Average Reward: -70.42\n",
            "Ep. 413, Ep.Timesteps 66, Episode Reward: -115.15, Moving Average Reward: -71.73\n",
            "Ep. 414, Ep.Timesteps 65, Episode Reward: -117.46, Moving Average Reward: -73.38\n",
            "Ep. 415, Ep.Timesteps 64, Episode Reward: -116.65, Moving Average Reward: -73.86\n",
            "Ep. 416, Ep.Timesteps 64, Episode Reward: -116.15, Moving Average Reward: -73.86\n",
            "Ep. 417, Ep.Timesteps 165, Episode Reward: -110.48, Moving Average Reward: -74.89\n",
            "Ep. 418, Ep.Timesteps 1600, Episode Reward: -14.89, Moving Average Reward: -74.44\n",
            "Ep. 419, Ep.Timesteps 1600, Episode Reward: -33.22, Moving Average Reward: -74.38\n",
            "Ep. 420, Ep.Timesteps 1600, Episode Reward: -49.82, Moving Average Reward: -74.90\n",
            "Ep. 421, Ep.Timesteps 1600, Episode Reward: -45.56, Moving Average Reward: -74.88\n",
            "Ep. 422, Ep.Timesteps 1600, Episode Reward: -28.66, Moving Average Reward: -73.24\n",
            "Ep. 423, Ep.Timesteps 1600, Episode Reward: -44.11, Moving Average Reward: -72.64\n",
            "Ep. 424, Ep.Timesteps 1600, Episode Reward: -30.38, Moving Average Reward: -71.05\n",
            "Ep. 425, Ep.Timesteps 1600, Episode Reward: -43.85, Moving Average Reward: -71.58\n",
            "Ep. 426, Ep.Timesteps 1600, Episode Reward: -55.74, Moving Average Reward: -71.56\n",
            "Ep. 427, Ep.Timesteps 139, Episode Reward: -103.39, Moving Average Reward: -71.50\n",
            "Ep. 428, Ep.Timesteps 1600, Episode Reward: -22.11, Moving Average Reward: -70.22\n",
            "Ep. 429, Ep.Timesteps 375, Episode Reward: -76.25, Moving Average Reward: -69.88\n",
            "Ep. 430, Ep.Timesteps 1600, Episode Reward: 95.27, Moving Average Reward: -65.58\n",
            "Ep. 431, Ep.Timesteps 110, Episode Reward: -122.73, Moving Average Reward: -66.17\n",
            "Ep. 432, Ep.Timesteps 132, Episode Reward: -125.87, Moving Average Reward: -67.02\n",
            "Ep. 433, Ep.Timesteps 141, Episode Reward: -130.83, Moving Average Reward: -68.50\n",
            "Ep. 434, Ep.Timesteps 1373, Episode Reward: -18.60, Moving Average Reward: -67.53\n",
            "Ep. 435, Ep.Timesteps 442, Episode Reward: -62.00, Moving Average Reward: -67.88\n",
            "Ep. 436, Ep.Timesteps 69, Episode Reward: -111.64, Moving Average Reward: -69.29\n",
            "Ep. 437, Ep.Timesteps 1600, Episode Reward: -66.01, Moving Average Reward: -68.70\n",
            "Ep. 438, Ep.Timesteps 1500, Episode Reward: 259.47, Moving Average Reward: -62.15\n",
            "Ep. 439, Ep.Timesteps 1600, Episode Reward: 79.93, Moving Average Reward: -58.57\n",
            "Ep. 440, Ep.Timesteps 1600, Episode Reward: 18.22, Moving Average Reward: -56.90\n",
            "Ep. 441, Ep.Timesteps 992, Episode Reward: 75.59, Moving Average Reward: -54.41\n",
            "Ep. 442, Ep.Timesteps 449, Episode Reward: -35.82, Moving Average Reward: -54.15\n",
            "Ep. 443, Ep.Timesteps 1600, Episode Reward: 156.46, Moving Average Reward: -49.77\n",
            "Ep. 444, Ep.Timesteps 1161, Episode Reward: 265.94, Moving Average Reward: -43.54\n",
            "Ep. 445, Ep.Timesteps 981, Episode Reward: 74.45, Moving Average Reward: -40.71\n",
            "Ep. 446, Ep.Timesteps 964, Episode Reward: 131.18, Moving Average Reward: -36.82\n",
            "Ep. 447, Ep.Timesteps 308, Episode Reward: -53.29, Moving Average Reward: -35.28\n",
            "Ep. 448, Ep.Timesteps 634, Episode Reward: 21.62, Moving Average Reward: -33.70\n",
            "Ep. 449, Ep.Timesteps 707, Episode Reward: 39.83, Moving Average Reward: -29.57\n",
            "Ep. 450, Ep.Timesteps 1369, Episode Reward: 260.76, Moving Average Reward: -23.53\n",
            "Ep. 451, Ep.Timesteps 1600, Episode Reward: 178.21, Moving Average Reward: -19.06\n",
            "Ep. 452, Ep.Timesteps 1084, Episode Reward: 275.82, Moving Average Reward: -12.42\n",
            "Ep. 453, Ep.Timesteps 163, Episode Reward: -66.58, Moving Average Reward: -12.32\n",
            "Ep. 454, Ep.Timesteps 365, Episode Reward: -14.22, Moving Average Reward: -10.71\n",
            "Ep. 455, Ep.Timesteps 1093, Episode Reward: 279.09, Moving Average Reward: -4.52\n",
            "Ep. 456, Ep.Timesteps 1118, Episode Reward: 279.02, Moving Average Reward: 1.87\n",
            "Ep. 457, Ep.Timesteps 1098, Episode Reward: 280.58, Moving Average Reward: 9.19\n",
            "Ep. 458, Ep.Timesteps 1078, Episode Reward: 282.46, Moving Average Reward: 15.60\n",
            "Ep. 459, Ep.Timesteps 1156, Episode Reward: 279.79, Moving Average Reward: 23.36\n",
            "Ep. 460, Ep.Timesteps 1119, Episode Reward: 280.32, Moving Average Reward: 29.28\n",
            "Ep. 461, Ep.Timesteps 1016, Episode Reward: 288.18, Moving Average Reward: 36.88\n",
            "Ep. 462, Ep.Timesteps 1072, Episode Reward: 280.58, Moving Average Reward: 45.02\n",
            "Ep. 463, Ep.Timesteps 332, Episode Reward: -32.01, Moving Average Reward: 46.69\n",
            "Ep. 464, Ep.Timesteps 1062, Episode Reward: 286.56, Moving Average Reward: 54.77\n",
            "Ep. 465, Ep.Timesteps 1043, Episode Reward: 285.43, Moving Average Reward: 62.81\n",
            "Ep. 466, Ep.Timesteps 1034, Episode Reward: 284.67, Moving Average Reward: 70.83\n",
            "Ep. 467, Ep.Timesteps 1028, Episode Reward: 279.55, Moving Average Reward: 78.63\n",
            "Ep. 468, Ep.Timesteps 511, Episode Reward: 40.07, Moving Average Reward: 79.73\n",
            "Ep. 469, Ep.Timesteps 1027, Episode Reward: 285.17, Moving Average Reward: 86.09\n",
            "Ep. 470, Ep.Timesteps 1004, Episode Reward: 283.34, Moving Average Reward: 92.76\n",
            "Ep. 471, Ep.Timesteps 997, Episode Reward: 285.16, Moving Average Reward: 99.37\n",
            "Ep. 472, Ep.Timesteps 1096, Episode Reward: 274.10, Moving Average Reward: 105.43\n",
            "Ep. 473, Ep.Timesteps 964, Episode Reward: 285.46, Moving Average Reward: 112.02\n",
            "Ep. 474, Ep.Timesteps 945, Episode Reward: 288.05, Moving Average Reward: 118.39\n",
            "Ep. 475, Ep.Timesteps 957, Episode Reward: 287.04, Moving Average Reward: 125.00\n",
            "Ep. 476, Ep.Timesteps 966, Episode Reward: 289.19, Moving Average Reward: 131.90\n",
            "Ep. 477, Ep.Timesteps 929, Episode Reward: 291.24, Moving Average Reward: 139.80\n",
            "Ep. 478, Ep.Timesteps 912, Episode Reward: 290.58, Moving Average Reward: 146.05\n",
            "Ep. 479, Ep.Timesteps 946, Episode Reward: 286.97, Moving Average Reward: 153.31\n",
            "Ep. 480, Ep.Timesteps 940, Episode Reward: 284.41, Moving Average Reward: 157.10\n",
            "Ep. 481, Ep.Timesteps 912, Episode Reward: 284.94, Moving Average Reward: 165.25\n",
            "Ep. 482, Ep.Timesteps 438, Episode Reward: 24.30, Moving Average Reward: 168.25\n",
            "Ep. 483, Ep.Timesteps 953, Episode Reward: 288.57, Moving Average Reward: 176.64\n",
            "Ep. 484, Ep.Timesteps 985, Episode Reward: 288.89, Moving Average Reward: 182.79\n"
          ]
        }
      ],
      "source": [
        "# The following code is provided for the training of your agent in the 'BipedalWalker-v3' gym environment.\n",
        "gym.logger.set_level(40)\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "_ = env.reset()\n",
        "env.reset(seed=0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "timesteps_count = 0  # Counting the time steps\n",
        "max_steps = 1600  # Maximum time steps for one episode\n",
        "ep_reward_list = deque(maxlen=50)\n",
        "avg_reward = -9999\n",
        "agent = TD3()\n",
        "\n",
        "for ep in range(600):\n",
        "    state, info = env.reset()\n",
        "    episodic_reward = 0\n",
        "    timestep_for_cur_episode = 0\n",
        "\n",
        "    for st in range(max_steps):\n",
        "        # Select action according to policy\n",
        "        action = agent.policy(state)\n",
        "\n",
        "        # Recieve state and reward from environment.\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        episodic_reward += reward\n",
        "\n",
        "        # Send the experience to the agent and train the agent\n",
        "        agent.train(timesteps_count, timestep_for_cur_episode, state, action, reward, next_state, done)\n",
        "\n",
        "        timestep_for_cur_episode += 1\n",
        "        timesteps_count += 1\n",
        "\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "    print('Ep. {}, Ep.Timesteps {}, Episode Reward: {:.2f}'.format(ep + 1, timestep_for_cur_episode, episodic_reward), end='')\n",
        "\n",
        "    if len(ep_reward_list) == 50:\n",
        "        # Mean of last 50 episodes\n",
        "        avg_reward = sum(ep_reward_list) / 50\n",
        "        print(', Moving Average Reward: {:.2f}'.format(avg_reward))\n",
        "    else:\n",
        "        print('')\n",
        "\n",
        "print('Average reward over 50 episodes: ', avg_reward)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the actor\n",
        "actor_path = \"actor.pth\"\n",
        "torch.save(agent.actor.to(\"cpu\").state_dict(), actor_path)"
      ],
      "metadata": {
        "id": "RosAJ5m1UvyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}